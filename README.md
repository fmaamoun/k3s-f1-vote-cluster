# Cloud-Native DevOps Architecture Showcase

Ce projet est une d√©monstration technique d'une architecture Cloud Native & DevOps de bout en bout. Il mat√©rialise les concepts d'Infrastructure-as-Code, d'orchestration et d'automatisation CI/CD pour garantir la r√©silience d'un service critique (application de vote en temps r√©el) sur un environnement cloud auto-g√©r√©.

![Status](https://img.shields.io/github/actions/workflow/status/fmaamoun/k3s-f1-vote-cluster/deploy.yml?label=Pipeline&logo=github)
![AWS](https://img.shields.io/badge/AWS-Infrastructure-FF9900?logo=amazon-aws&logoColor=white)
![Terraform](https://img.shields.io/badge/Terraform-v1.5+-purple?logo=terraform)
![Ansible](https://img.shields.io/badge/Ansible-2.14+-red?logo=ansible)
![Kubernetes](https://img.shields.io/badge/K3s-Cluster-blue?logo=kubernetes)
![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-CI%2FCD-2088FF?logo=github-actions&logoColor=white)

## üèóÔ∏è Architecture Technique

### 1. Infrastructure (AWS & Terraform)
*   **R√©seau (VPC)** :
    *   **Public Zone (DMZ)** : R√©partie sur 2 AZs (eu-west-3a/b) h√©bergeant l'Application Load Balancer (ALB) pour la haute disponibilit√© et le Bastion Host.
    *   **Private Zone (The Vault)** : Sous-r√©seau isol√© h√©bergeant le cluster Kubernetes (Master + Workers), inaccessible directement depuis Internet.
    *   **Gateways** : Internet Gateway (IGW) pour le trafic public et **NAT Gateway** pour permettre aux n≈ìuds priv√©s de t√©l√©charger des mises √† jour/images de mani√®re s√©curis√©e sans √™tre expos√©s.
*   **Security (Firewalls)** : Strat√©gie de **Security Groups** "Least Privilege" :
    *   **Bastion SG** : Seul point d'entr√©e SSH (Port 22) ouvert sur le monde.
    *   **ALB SG** : Filtre le trafic Web entrant et ne le redirige vers le cluster que s'il est l√©gitime.
    *   **Internal SG** : Verrouillage total des n≈ìuds K3s. Ils n'acceptent que le trafic venant du Bastion (SSH/API), de l'ALB (HTTP), et entre eux (VXLAN/Flannel).
*   **Compute (EC2)** :
    *   **Bastion** : Instance proxy (`t3.micro`) pour l'administration s√©curis√©e.
    *   **Master Node** : Instance de contr√¥le (`t3.small`) g√©rant l'API Server et la base de donn√©es cluster.
    *   **Worker Nodes** : Flotte de 2 instances (`t3.micro`) d√©di√©es √† l'ex√©cution des conteneurs applicatifs.
*   **Traffic Management (ALB)** : ALB AWS op√©rant au Layer 7, g√©rant la terminaison SSL (optionnelle) et routant le trafic HTTP vers les ports 80 des Workers (o√π √©coute l'Ingress Controller Traefik).
*   **Automation & Intelligence** :
    *   **Dynamic OS** : Utilisation dynamique de la derni√®re AMI **Ubuntu 22.04 LTS** disponible pour garantir un OS patch√© et s√©curis√©.
    *   **IaC Glueing** : Terraform g√©n√®re automatiquement l'inventaire Ansible et les cl√©s SSH, supprimant toute intervention manuelle entre le provisioning et la configuration.

### 2. Orchestration & Cluster (Kubernetes/K3s)
*   **Distribution** : **K3s**, distribution Kubernetes l√©g√®re certifi√©e CNCF.
*   **Ingress Controller** : **Traefik** (natif K3s) assurant le routage interne vers les services.
*   **Placement des Workloads** :
    *   **Redis** : Pinned sur le **Master Node** via `nodeSelector` pour isoler la donn√©e du calcul intensif.
    *   **App (F1 Vote)** : Pinned sur les **Worker Nodes** via `nodeAffinity` (Anti-affinity Control-Plane).
*   **Scaling** : HPA (Horizontal Pod Autoscaler) configur√© pour scaler les pods de **2 √† 8 r√©plicas** suivant l'utilisation CPU.

### 3. Configuration (Ansible)
*   **Bootstrapping** : Installation automatis√©e de K3s.
*   **Join Token** : R√©cup√©ration dynamique du token sur le Master et propagation s√©curis√©e aux Workers pour l'assemblage du cluster.
*   **OS Hardening** : Pr√©-configuration des paquets essentiels et mises √† jour de s√©curit√©.

### 4. CI/CD (GitHub Actions)
*   **Smart Pipeline** : D√©tection intelligente des changements (Paths Filter) pour d√©clencher le build Docker uniquement si le code source change.
*   **Secure Deployment** : D√©ploiement via **SSH Tunneling (ProxyCommand)** √† travers le Bastion pour atteindre le Master priv√©.
*   **Zero-Downtime** : Utilisation de `kubectl rollout status` pour garantir que la nouvelle version est saine avant de terminer le d√©ploiement.
*   **Registry** : Utilisation de **GHCR** (GitHub Container Registry) pour stocker les images docker tagg√©es par SHA de commit.

## üì¶ Workload Applicatif

L'application d√©ploy√©e ("F1 Voting App") sert de t√©moin pour valider la r√©silience de l'infrastructure. Le sc√©nario retenu simule le vote "Driver of the Day" de la Formule 1, un cas d'usage caract√©ris√© par des pics de charge intenses et soudains (Burst Traffic) en fin de course. Elle est compos√©e de deux micro-services :
* **Frontend/Backend** : SvelteKit (Node.js) g√©rant l'interface et l'API.
* **Data Store** : Redis pour la persistance volatile haute performance.

**Fonctionnalit√©s expos√©es :**
* **Route Publique (`/`)** : Interface utilisateur connect√©e via WebSocket pour le vote temps r√©el.
* **Route Administration (`/admin`)** : Interface de pilotage permettant de modifier l'√©tat du syst√®me (Ouverture/Fermeture des votes, Reset) et de visualiser les m√©triques Redis en direct.

## üöÄ Guide de D√©ploiement

Cette proc√©dure permet de r√©pliquer l'int√©gralit√© de l'infrastructure sur un compte AWS vierge.

### 1. Pr√©-requis
* Un compte AWS avec acc√®s programmatique (Access Key/Secret Key).
* Terraform & Ansible install√©s sur la machine de contr√¥le.

### 2. Provisioning Infrastructure (Terraform)
Initialisation et application du plan d'infrastructure :
```bash
cd terraform
terraform init
terraform apply
```

### 3. Configuration Cluster (Ansible)

Mise √† jour de l'inventaire avec les IPs provisionn√©es et ex√©cution du playbook :

```bash
cd ansible
ansible-playbook -i inventory.ini playbook.yml
```

### 4. Configuration CI/CD (GitHub)

Configurer les secrets suivants dans le d√©p√¥t pour permettre au pipeline de piloter le cluster :

| Secret | Description |
| --- | --- |
| `MASTER_HOST` | IP Priv√©e du n≈ìud Master (Accessible via Bastion) |
| `BASTION_HOST` | IP Publique du Bastion |
| `SSH_PRIVATE_KEY` | Contenu de la cl√© priv√©e SSH utilis√©e par Ansible |

**Variable d'environnement :**

* `APP_URL` : DNS du Load Balancer AWS (requis pour les environnements de d√©ploiement).

### 5. Lancement

Pour le premier d√©ploiement, rendez-vous dans l'onglet **Actions** de GitHub, s√©lectionnez le workflow **"Production Deployment"** et cliquez sur **Run workflow**.

Par la suite, tout commit pouss√© sur la branche `main` impactant l'application (`app/`) ou les manifestes Kubernetes (`kubernetes/`) d√©clenchera automatiquement le pipeline de mise √† jour.

> [!WARNING]
> **Cost Management :** Cette infrastructure utilise des ressources AWS r√©elles (EC2, VPC, ELB). Pour √©viter des co√ªts inutiles apr√®s utilisation, n'oubliez pas de d√©truire les ressources :
> ```bash
> cd terraform && terraform destroy
> ```

## üõ†Ô∏è Stack Technologique

### DevOps & Cloud
*   **AWS** : Infrastructure & Services Cloud Natifs.
*   **Terraform** : Infrastructure as Code.
*   **Ansible** : Configuration Management.
*   **Kubernetes (K3s)** : Orchestration de conteneurs.
*   **GitHub Actions** : CI/CD.
*   **Docker** : Container Registry.

### Application Components
*   **SvelteKit** : Framework Frontend & API.
*   **Redis** : Base de donn√©es cl√©-valeur.
*   **TailwindCSS** : Utilitaire CSS.
*   **WebSocket** : Communication temps r√©el.